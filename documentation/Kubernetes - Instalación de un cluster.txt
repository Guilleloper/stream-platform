##########################################
# KUBERNETES - INSTALACIÓN DE UN CLUSTER #
##########################################


1. INSTALACIÓN COMÚN EN TODOS LOS NODOS DEL CLUSTER
2. INSTALACIÓN NODO MASTER
3. INSTALACIÓN NODOS WORKER
4. COMPROBACIONES
5. COSAS PENDIENTES

-------------------------------------------------------------------------------------------------------------------------------------------------------------

1. INSTALACIÓN COMÚN EN TODOS LOS NODOS DEL CLUSTER


- Configurar fichero hosts:
  # view /etc/hosts
  ...
  # Stream Platform servers
  10.26.0.1 master
  10.26.0.2 worker1
  10.26.0.3 worker2
  10.26.0.4 worker3
  10.26.0.5 worker4
  10.26.0.6 worker5
  10.26.0.7 worker6
  ...

- Añadir el repositorio de kubernetes:
  # cat <<EOF > /etc/yum.repos.d/kubernetes.repo
  [kubernetes]
  name=Kubernetes
  baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
  enabled=1
  gpgcheck=1
  repo_gpgcheck=1
  gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
  EOF

- Detener selinux:
  # setenforce 0

- Instalación del software:
  # yum install -y docker kubelet kubeadm kubectl etcd

- Editar la configuración de Docker para modificar el directorio de ejecución, y evitar futuros problemas de espacio:
  # view /etc/docker/daemon.json
  {
      "graph": "/var/lib/data/docker",
      "storage-driver": "overlay"
  }

- Parar y deshabilitar FW:
  # systemctl stop firewalld
  # systemctl disable firewalld

- Comprobar parámetro de Kernel relativo a la red y el iptables:
  # sysctl net.bridge.bridge-nf-call-iptables
  net.bridge.bridge-nf-call-iptables = 1

  Si estuviera a 0 habría que configurarlo:
  # view /etc/sysctl.d/k8s.conf
  ...
  net.bridge.bridge-nf-call-ip6tables = 1
  net.bridge.bridge-nf-call-iptables = 1
  # sysctl --system

- Comprobar que Docker y Kubernetes usan el mismo driver Cgroup:
  # docker info | grep -i cgroup
  # grep cgroup-driver /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

  Si fuera distinto, habría que cambiar el Cgroup en la configuración del servicio de kubelet:
  # sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
  # systemctl daemon-reload

- Habilitar y arrancar el servicio Docker:
  # systemctl start docker && systemctl enable docker

- Modificar configuración del servicio kubelet:
  # view /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
  ...
  Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice"
  ...

- Habilitar y arrancar el servicio Kubernetes:
  # systemctl daemon-reload
  # systemctl start kubelet && systemctl enable kubelet

- Deshabilitar el uso de swap:
  # swapoff -a 

- Deshabilitar el montaje de la swap (deshabilitar el uso de swap de forma permanente):
  # view /etc/fstab
  ...
  #
  # Se deshabilita el uso de swap
  #/dev/mapper/vg_root-lv_swap swap                    swap    defaults        0 0
  ...

- Obtener las imágenes de docker que serán necesarias:
  (desde un equipo con conexión a Internet)
  # sudo docker pull k8s.gcr.io/kube-apiserver-amd64:v1.10.2
  # sudo docker pull k8s.gcr.io/kube-controller-manager-amd64:v1.10.2
  # sudo docker pull k8s.gcr.io/kube-scheduler-amd64:v1.10.2
  # sudo docker pull k8s.gcr.io/etcd-amd64:3.1.12
  # sudo docker pull k8s.gcr.io/pause-amd64:3.1
  # sudo docker pull k8s.gcr.io/kube-proxy-amd64:v1.10.2
  # sudo docker pull quay.io/calico/cni:v1.11.0
  # sudo docker pull quay.io/calico/node:v2.6.2
  # sudo docker pull quay.io/coreos/flannel:v0.9.1
  # sudo docker pull k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.8
  # sudo docker pull k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.8
  # sudo docker pull k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.8
  # sudo docker save k8s.gcr.io/kube-apiserver-amd64:v1.10.2 > kube-apiserver-amd64_v1.10.2.tar
  # sudo docker save k8s.gcr.io/kube-controller-manager-amd64:v1.10.2 > kube-controller-manager-amd64_v1.10.2.tar
  # sudo docker save k8s.gcr.io/kube-scheduler-amd64:v1.10.2 > kube-scheduler-amd64_v1.10.2.tar
  # sudo docker save k8s.gcr.io/etcd-amd64:3.1.12 > etcd-amd64_3.1.12.tar
  # sudo docker save k8s.gcr.io/pause-amd64:3.1 > pause-amd64_3.1.tar
  # sudo docker save k8s.gcr.io/kube-proxy-amd64:v1.10.2 > kube-proxy-amd64_v1.10.2.tar
  # sudo docker save quay.io/calico/cni:v1.11.0 > cni_v1.11.0.tar
  # sudo docker save quay.io/calico/node:v2.6.2 > node_v2.6.2.tar
  # sudo docker save quay.io/coreos/flannel:v0.9.1 > flannel_v0.9.1.tar
  # sudo docker save k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.8 > k8s-dns-kube-dns-amd64_1.14.8.tar
  # sudo docker save k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.8 > k8s-dns-dnsmasq-nanny-amd64_1.14.8.tar
  # sudo docker save k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.8 > k8s-dns-sidecar-amd64_1.14.8.tar

- Subir las imágenes de docker a los servidores:
  # md5sum /var/lib/data/kube-apiserver-amd64_v1.10.2.tar
  aa50d636a4fb7986c351529a0a96a1ea  /var/lib/data/kube-apiserver-amd64_v1.10.2.tar
  # md5sum /var/lib/data/kube-controller-manager-amd64_v1.10.2.tar
  f72547a0c96ff1ac72d0afce9645ab0f  /var/lib/data/kube-controller-manager-amd64_v1.10.2.tar
  # md5sum /var/lib/data/kube-scheduler-amd64_v1.10.2.tar
  90b2e5891f9f51e0e72a2903e45a3533  /var/lib/data/kube-scheduler-amd64_v1.10.2.tar
  # md5sum /var/lib/data/etcd-amd64_3.1.12.tar
  b81ea7964bbc31072d23280cae6af636  /var/lib/data/etcd-amd64_3.1.12.tar
  # md5sum /var/lib/data/pause-amd64_3.1.tar
  5874b3bdaaeb781f86766589d0c1fc59  /var/lib/data/pause-amd64_3.1.tar
  # md5sum /var/lib/data/kube-proxy-amd64_v1.10.2.tar
  9259d645021cb5aa265941bbd69d0635  /var/lib/data/kube-proxy-amd64_v1.10.2.tar
  # md5sum /var/lib/data/cni_v1.11.0.tar
  c4a14eb4a4e425b16e93a18f0881f62f  /var/lib/data/cni_v1.11.0.tar
  # md5sum /var/lib/data/node_v2.6.2.tar
  94345b62e27c5e51bf30472d86235c9d  /var/lib/data/node_v2.6.2.tar
  # md5sum /var/lib/data/flannel_v0.9.1.tar
  392e1c065a654772eb099c4898ca8e4f  /var/lib/data/flannel_v0.9.1.tar
  # md5sum /var/lib/data/k8s-dns-kube-dns-amd64_1.14.8.tar
  79cd18ce8a6c09428db4b4e0e9641033  /var/lib/data/k8s-dns-kube-dns-amd64_1.14.8.tar
  # md5sum /var/lib/data/k8s-dns-dnsmasq-nanny-amd64_1.14.8.tar
  7ba466cb0d1668245a3db51d5369ebfb  /var/lib/data/k8s-dns-dnsmasq-nanny-amd64_1.14.8.tar
  # md5sum /var/lib/data/k8s-dns-sidecar-amd64_1.14.8.tar
  3e3a91fcd8d29b39ea46017a5e5e3426  /var/lib/data/k8s-dns-sidecar-amd64_1.14.8.tar

-------------------------------------------------------------------------------------------------------------------------------------------------------------

2. INSTALACIÓN NODO MASTER

- Cargar las imágenes de docker:
  # docker load < /var/lib/data/kube-apiserver-amd64_v1.10.2.tar
  # docker load < /var/lib/data/kube-controller-manager-amd64_v1.10.2.tar
  # docker load < /var/lib/data/kube-scheduler-amd64_v1.10.2.tar
  # docker load < /var/lib/data/etcd-amd64_3.1.12.tar
  # docker load < /var/lib/data/pause-amd64_3.1.tar
  # docker load < /var/lib/data/kube-proxy-amd64_v1.10.2.tar
  # docker load < /var/lib/data/cni_v1.11.0.tar
  # docker load < /var/lib/data/node_v2.6.2.tar
  # docker load < /var/lib/data/flannel_v0.9.1.tar
  # docker load < /var/lib/data/k8s-dns-kube-dns-amd64_1.14.8.tar
  # docker load < /var/lib/data/k8s-dns-dnsmasq-nanny-amd64_1.14.8.tar
  # docker load < /var/lib/data/k8s-dns-sidecar-amd64_1.14.8.tar

- Inicializar el master:
  # kubeadm init --kubernetes-version v1.10.2 --pod-network-cidr=10.244.0.0/16

- Ubicar el fichero de configuración para ser usado por el usuario root:
  # cp /etc/kubernetes/admin.conf $HOME/
  # chown $(id -u):$(id -g) $HOME/admin.conf
  # export KUBECONFIG=$HOME/admin.conf

- Modificar profile del usuario root:
  # view $HOME/.bash_profile
  ...
  # Kubernetes variables
  export KUBECONFIG=$HOME/admin.conf
  ...

- Habilitar la funcionalidad de autocompletado, para el comando kubectl:
  # echo "source <(kubectl completion bash)" >> ~/.bashrc

- Subir los ficheros de configuración de los CNI "Canal":
  # md5sum /var/tmp/rbac.yaml
  dbf54cab81791063ac6d16fceb828366  /var/tmp/rbac.yaml
  # md5sum /var/tmp/canal.yaml
  a1aa88bd6c4464993450b6e0bce7d0b0  /var/tmp/canal.yaml

- Aplicar plugin CNI "Canal":
  # kubectl apply -f /var/tmp/rbac.yaml
  # kubectl apply -f /var/tmp/canal.yaml

-------------------------------------------------------------------------------------------------------------------------------------------------------------

3. INSTALACIÓN NODOS WORKER

- Cargar las imágenes de docker:
  # docker load < /var/lib/data/pause-amd64_3.1.tar
  # docker load < /var/lib/data/kube-proxy-amd64_v1.10.2.tar
  # docker load < /var/lib/data/cni_v1.11.0.tar
  # docker load < /var/lib/data/node_v2.6.2.tar
  # docker load < /var/lib/data/flannel_v0.9.1.tar
  # docker load < /var/lib/data/k8s-dns-kube-dns-amd64_1.14.8.tar
  # docker load < /var/lib/data/k8s-dns-dnsmasq-nanny-amd64_1.14.8.tar
  # docker load < /var/lib/data/k8s-dns-sidecar-amd64_1.14.8.tar

- Incluir cada worker en el cluster:
  (con las opciones de token y hash que haya dado por salida la ejecución del comando kubeadm init, ejecutado en el nodo master)
  # kubeadm join 10.26.223.17:6443 --token gb48co.xj6bi0jsu1nmm077 --discovery-token-ca-cert-hash sha256:8a7ea351230b16c20a2bc3eeb708b7d7261ee4799291a68653717ef5e3056c82

-------------------------------------------------------------------------------------------------------------------------------------------------------------

4. COMPROBACIONES

- Comprobar la correcta inclusión de los nodos Worker en el cluster:
  # kubectl get nodes

- Comprobar la correcta ejecución de los pods internos del cluster:
  # kubectl get po --all-namespaces -o wide

-------------------------------------------------------------------------------------------------------------------------------------------------------------

5. COSAS PENDIENTES

- Hacer correr los procesos o servicio de Kubernetes con un usuario propio.
- Identificar las comunicaciones internas entre nodos, habilitar el servicio firewalld y configurar las correspondientes reglas.


